{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "MODEL_DIR = '../models/siamese'\n",
    "LOG_DIR = '../logs/04chest/'\n",
    "SIZE = (224, 224)\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.mkdir(MODEL_DIR)\n",
    "caminho_bases = os.path.join('..', 'bases', 'chest_xray')\n",
    "caminho_train = os.path.join(caminho_bases, 'train')\n",
    "caminho_test = os.path.join(caminho_bases, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, \\\n",
    "    ReduceLROnPlateau, TensorBoard\n",
    "\n",
    "tensorboard_logs = TensorBoard(log_dir=LOG_DIR, histogram_freq=1,\n",
    "                               write_graph=False, write_images=False,\n",
    "                               update_freq='epoch')\n",
    "mcp_save = ModelCheckpoint(os.path.join(MODEL_DIR, \n",
    "                                        '04modelweights.{epoch:02d}-{val_loss:.2f}.hdf5'),\n",
    "                           save_best_only=True, monitor='val_loss', mode='min')\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2,\n",
    "                              verbose=1, mode='min')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 224, 224, 16)      448       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 112, 112, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 112, 112, 32)      4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 56, 56, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 56, 56, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 56, 56, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 28, 28, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 12, 12, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 4, 4, 256)         295168    \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               524416    \n",
      "=================================================================\n",
      "Total params: 1,064,608\n",
      "Trainable params: 1,064,608\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Activation, Conv2D, \\\n",
    "    BatchNormalization, concatenate, Dense, Dropout, Flatten, Input, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "\n",
    "SIZE = (224, 224)\n",
    "\n",
    "\n",
    "nuclear_model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Conv2D(16, (3, 3),\n",
    "                         padding='same',\n",
    "                         activation='relu',\n",
    "                         input_shape=(*SIZE, 3)),\n",
    "  MaxPooling2D(pool_size=(2, 2)),\n",
    "  Conv2D(32, (3, 3), padding='same', activation='relu'),\n",
    "  MaxPooling2D(pool_size=(2, 2)),\n",
    "  Dropout(0.2),\n",
    "  Conv2D(64, (3, 3), padding='same', activation='relu'),\n",
    "  MaxPooling2D(pool_size=(2, 2)),\n",
    "  Dropout(0.25),\n",
    "  Conv2D(128, (3, 3), padding='same', activation='relu'),\n",
    "  MaxPooling2D(pool_size=(2, 2)),\n",
    "  Dropout(0.25),\n",
    "  Conv2D(128, (3, 3), activation='relu'),\n",
    "  MaxPooling2D(pool_size=(2, 2)),\n",
    "  Dropout(0.25),\n",
    "  Conv2D(256, (3, 3), activation='relu'),\n",
    "  Flatten(),\n",
    "  Dense(128, activation='relu'),\n",
    " \n",
    "])\n",
    "\n",
    "nuclear_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Similarity_Model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential (Sequential)         (None, 128)          1064608     input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "merge_features (Concatenate)    (None, 256)          0           sequential[1][0]                 \n",
      "                                                                 sequential[2][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 16)           4112        merge_features[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 16)           64          dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 16)           0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 4)            68          activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 4)            16          dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 4)            0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            5           activation_1[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 1,068,873\n",
      "Trainable params: 1,068,833\n",
      "Non-trainable params: 40\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "a_in = Input(shape=(*SIZE, 3))\n",
    "b_in = Input(shape=(*SIZE, 3))\n",
    "\n",
    "a_feat = nuclear_model(a_in)\n",
    "b_feat = nuclear_model(b_in)\n",
    "\n",
    "combined_features = concatenate([a_feat, b_feat], name = 'merge_features')\n",
    "combined_features = Dense(16, activation = 'linear')(combined_features)\n",
    "combined_features = BatchNormalization()(combined_features)\n",
    "combined_features = Activation('relu')(combined_features)\n",
    "combined_features = Dense(4, activation = 'linear')(combined_features)\n",
    "combined_features = BatchNormalization()(combined_features)\n",
    "combined_features = Activation('relu')(combined_features)\n",
    "combined_features = Dense(1, activation = 'sigmoid')(combined_features)\n",
    "similarity_model = tf.keras.Model(inputs = [a_in, b_in], outputs = [combined_features], name = 'Similarity_Model')\n",
    "similarity_model.summary()\n",
    "\n",
    "similarity_model.compile(optimizer=Adam(lr=0.001), loss = 'binary_crossentropy', metrics = ['mae'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "caminho_normal_train = os.path.join(caminho_train, 'NORMAL')\n",
    "caminho_pneumonia_train = os.path.join(caminho_train, 'PNEUMONIA')\n",
    "caminho_normal_test = os.path.join(caminho_test, 'NORMAL')\n",
    "caminho_pneumonia_test = os.path.join(caminho_test, 'PNEUMONIA')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def generate_random_batch(nvazios_list, vazios_list, batch_size=64):\n",
    "    def get_item(classe):\n",
    "        if classe == 0:\n",
    "            return nvazios_list.pop()\n",
    "        return vazios_list.pop()\n",
    "    result = []\n",
    "    for i in range(batch_size):\n",
    "        classe1 = random.randint(0, 1)\n",
    "        item1 = get_item(classe1) \n",
    "        classe2 = random.randint(0, 1)\n",
    "        item2 = get_item(classe2)\n",
    "        if classe1 == classe2:\n",
    "            if item1 == item2:\n",
    "                alpha = 0\n",
    "            else:\n",
    "                alpha = 0.001\n",
    "        else:\n",
    "            alpha = 1\n",
    "        result.append((item1, item2, alpha))\n",
    "    return result  \n",
    "\n",
    "def image_generator(caminho1, caminho2, batch_size=32):\n",
    "    list_files1 = [filename for filename in os.listdir(caminho1)\n",
    "                   if filename[-4:] in  ['.jpg', 'jpeg', '.png']]\n",
    "    list_files2 = [filename for filename in os.listdir(caminho2)\n",
    "                   if filename[-4:] in  ['.jpg', 'jpeg', '.png']]\n",
    "    list1 = []\n",
    "    list2 = []\n",
    "    while True:\n",
    "        # Ciclar lista (se acabar, recarregar do come√ßo)\n",
    "        if len(list1) < batch_size * 2:\n",
    "            random.shuffle(list_files1)\n",
    "            list1 = [os.path.join(caminho1, arq) for arq in list_files1]\n",
    "        if len(list2) < batch_size * 2:\n",
    "            random.shuffle(list_files2)\n",
    "            list2 = [os.path.join(caminho2, arq) for arq in list_files2]\n",
    "        # Gerar um batch da lista\n",
    "        triples = generate_random_batch(list1, list2, batch_size)\n",
    "        X1 = np.zeros((batch_size, *SIZE, 3))\n",
    "        X2 = np.zeros((batch_size, *SIZE, 3))\n",
    "        y = np.zeros((batch_size, 1))\n",
    "        for ind, triple in enumerate(triples):\n",
    "            pil_img1 = Image.open(triple[0]).convert('RGB')\n",
    "            # print(pil_img1.size)\n",
    "            # print(pil_img1.mode)\n",
    "            # print(pil_img1.getbands())\n",
    "            pil_img1 = pil_img1.resize(SIZE, Image.ANTIALIAS)\n",
    "            pil_img2 = Image.open(triple[1]).convert('RGB')\n",
    "            pil_img2 = pil_img2.resize(SIZE, Image.ANTIALIAS)\n",
    "            label = triple[2]\n",
    "            X1[ind, :, :, :] = np.array(pil_img1) / 255.\n",
    "            X2[ind, :, :, :] = np.array(pil_img2) / 255.\n",
    "            y[ind, :] = label\n",
    "        \n",
    "        yield [X1, X2], y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = image_generator(caminho_normal_train, caminho_pneumonia_train)\n",
    "validation_generator = image_generator(caminho_normal_test, caminho_pneumonia_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 2\n",
    "val_loss = 0.69\n",
    "\n",
    "similarity_model.load_weights(os.path.join(\n",
    "    MODEL_DIR, \n",
    "    '04modelweights.{:02d}-{:.2f}.hdf5'.format(epoch, val_loss)\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0717 10:37:55.076833 139630067492608 deprecation.py:323] From /home/ivan/pybr/projeto/venv/lib/python3.5/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 1/50 [..............................] - ETA: 6:40 - loss: 0.7700 - mae: 0.5216"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0717 10:38:00.939146 139630067492608 callbacks.py:241] Method (on_train_batch_end) is slow compared to the batch update (0.197360). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 168s 3s/step - loss: 0.7204 - mae: 0.4995 - val_loss: 1.3654 - val_mae: 0.4978\n",
      "Epoch 2/100\n",
      "50/50 [==============================] - 137s 3s/step - loss: 0.7001 - mae: 0.4949 - val_loss: 0.8891 - val_mae: 0.4661\n",
      "Epoch 3/100\n",
      "50/50 [==============================] - 127s 3s/step - loss: 0.6595 - mae: 0.4715 - val_loss: 0.8021 - val_mae: 0.4844\n",
      "Epoch 4/100\n",
      "50/50 [==============================] - 124s 2s/step - loss: 0.5639 - mae: 0.4065 - val_loss: 0.8634 - val_mae: 0.4968\n",
      "Epoch 5/100\n",
      "49/50 [============================>.] - ETA: 1s - loss: 0.4976 - mae: 0.3644\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "50/50 [==============================] - 122s 2s/step - loss: 0.4980 - mae: 0.3644 - val_loss: 0.8404 - val_mae: 0.4879\n",
      "Epoch 6/100\n",
      "50/50 [==============================] - 123s 2s/step - loss: 0.4599 - mae: 0.3397 - val_loss: 1.2258 - val_mae: 0.4863\n",
      "Epoch 7/100\n",
      "49/50 [============================>.] - ETA: 1s - loss: 0.4430 - mae: 0.3299\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "50/50 [==============================] - 122s 2s/step - loss: 0.4452 - mae: 0.3308 - val_loss: 1.0133 - val_mae: 0.4755\n",
      "Epoch 8/100\n",
      "50/50 [==============================] - 120s 2s/step - loss: 0.4259 - mae: 0.3170 - val_loss: 1.1617 - val_mae: 0.4987\n",
      "Epoch 9/100\n",
      "49/50 [============================>.] - ETA: 1s - loss: 0.4142 - mae: 0.3111\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "50/50 [==============================] - 121s 2s/step - loss: 0.4139 - mae: 0.3109 - val_loss: 1.4272 - val_mae: 0.4861\n",
      "Epoch 10/100\n",
      "50/50 [==============================] - 120s 2s/step - loss: 0.4029 - mae: 0.3035 - val_loss: 1.8226 - val_mae: 0.5077\n",
      "Epoch 11/100\n",
      "49/50 [============================>.] - ETA: 1s - loss: 0.3883 - mae: 0.2966\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "50/50 [==============================] - 122s 2s/step - loss: 0.3897 - mae: 0.2974 - val_loss: 1.5821 - val_mae: 0.4921\n",
      "Epoch 12/100\n",
      "50/50 [==============================] - 123s 2s/step - loss: 0.3976 - mae: 0.3004 - val_loss: 1.3897 - val_mae: 0.5071\n",
      "Epoch 13/100\n",
      "49/50 [============================>.] - ETA: 1s - loss: 0.3970 - mae: 0.2997\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "50/50 [==============================] - 120s 2s/step - loss: 0.3965 - mae: 0.2995 - val_loss: 1.2762 - val_mae: 0.4705\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7efd583aab70>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_model.fit_generator(train_generator,\n",
    "                               steps_per_epoch=50,\n",
    "                               epochs=100,\n",
    "                               verbose=1,\n",
    "                               callbacks=[tensorboard_logs, mcp_save,\n",
    "                                         early_stop, reduce_lr],\n",
    "                               validation_data=validation_generator,\n",
    "                               validation_steps=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_gym2",
   "language": "python",
   "name": "venv_gym2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

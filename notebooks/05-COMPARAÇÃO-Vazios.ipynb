{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparação entre modelos para definir publicação ou não em produção\n",
    "## Comparar:\n",
    "* Accuracy geral\n",
    "* Recall em vazios\n",
    "* Recall em não vazios\n",
    "* Velocidade\n",
    "* Uso de memória"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from time import time\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carregar imagens (para modelos 2 e 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "caminho_basest = os.path.join('..', 'bases', 'vazios')\n",
    "caminho_traint = os.path.join(caminho_basest, 'train')\n",
    "caminho_testt = os.path.join(caminho_basest, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2317 images belonging to 2 classes.\n",
      "Found 20845 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "SIZE = (224, 224)\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    caminho_testt,\n",
    "    target_size=SIZE,\n",
    "    batch_size=128,\n",
    "    class_mode='binary',\n",
    "    shuffle=False)\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    caminho_traint,\n",
    "    target_size=SIZE,\n",
    "    batch_size=128,\n",
    "    class_mode='binary',\n",
    "    shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carregar modelo scvvazios.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2317 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "SIZE = (128, 128)\n",
    "MODEL_FILE = 'vaziossvc.pkl'\n",
    "\n",
    "validation_generator_svc = validation_datagen.flow_from_directory(\n",
    "    caminho_testt,\n",
    "    target_size=SIZE,\n",
    "    batch_size=128,\n",
    "    class_mode='binary',\n",
    "    shuffle=False)\n",
    "\n",
    "\n",
    "model1 = joblib.load(MODEL_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "0 (128, 128, 128, 3)\n",
      "CPU times: user 5.13 s, sys: 0 ns, total: 5.13 s\n",
      "Wall time: 5.13 s\n",
      "1 (128, 128, 128, 3)\n",
      "CPU times: user 5.02 s, sys: 0 ns, total: 5.02 s\n",
      "Wall time: 5.01 s\n",
      "2 (128, 128, 128, 3)\n",
      "CPU times: user 5.12 s, sys: 3.66 ms, total: 5.12 s\n",
      "Wall time: 5.12 s\n",
      "3 (128, 128, 128, 3)\n",
      "CPU times: user 5.24 s, sys: 0 ns, total: 5.24 s\n",
      "Wall time: 5.24 s\n",
      "4 (128, 128, 128, 3)\n",
      "CPU times: user 5.12 s, sys: 0 ns, total: 5.12 s\n",
      "Wall time: 5.12 s\n",
      "5 (128, 128, 128, 3)\n",
      "CPU times: user 5.11 s, sys: 0 ns, total: 5.11 s\n",
      "Wall time: 5.11 s\n",
      "6 (128, 128, 128, 3)\n",
      "CPU times: user 5.2 s, sys: 0 ns, total: 5.2 s\n",
      "Wall time: 5.2 s\n",
      "7 (128, 128, 128, 3)\n",
      "CPU times: user 5.12 s, sys: 0 ns, total: 5.12 s\n",
      "Wall time: 5.12 s\n",
      "8 (128, 128, 128, 3)\n",
      "CPU times: user 5.11 s, sys: 0 ns, total: 5.11 s\n",
      "Wall time: 5.11 s\n",
      "9 (128, 128, 128, 3)\n",
      "CPU times: user 5.06 s, sys: 0 ns, total: 5.06 s\n",
      "Wall time: 5.06 s\n",
      "10 (128, 128, 128, 3)\n",
      "CPU times: user 5.1 s, sys: 0 ns, total: 5.1 s\n",
      "Wall time: 5.1 s\n",
      "11 (128, 128, 128, 3)\n",
      "CPU times: user 5.11 s, sys: 0 ns, total: 5.11 s\n",
      "Wall time: 5.11 s\n",
      "12 (128, 128, 128, 3)\n",
      "CPU times: user 5.29 s, sys: 2.57 ms, total: 5.29 s\n",
      "Wall time: 5.29 s\n",
      "13 (128, 128, 128, 3)\n",
      "CPU times: user 5.18 s, sys: 3.89 ms, total: 5.19 s\n",
      "Wall time: 5.19 s\n",
      "14 (128, 128, 128, 3)\n",
      "CPU times: user 5.05 s, sys: 7.94 ms, total: 5.06 s\n",
      "Wall time: 5.06 s\n",
      "15 (128, 128, 128, 3)\n",
      "CPU times: user 5.04 s, sys: 3.92 ms, total: 5.04 s\n",
      "Wall time: 5.04 s\n",
      "16 (128, 128, 128, 3)\n",
      "CPU times: user 5.04 s, sys: 0 ns, total: 5.04 s\n",
      "Wall time: 5.04 s\n",
      "17 (128, 128, 128, 3)\n",
      "CPU times: user 5.02 s, sys: 11.9 ms, total: 5.04 s\n",
      "Wall time: 5.03 s\n",
      "18 (13, 128, 128, 3)\n",
      "CPU times: user 510 ms, sys: 16 µs, total: 510 ms\n",
      "Wall time: 509 ms\n",
      "CPU times: user 1min 36s, sys: 90.6 ms, total: 1min 36s\n",
      "Wall time: 1min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_pred1 = []\n",
    "print(len(validation_generator_svc))\n",
    "for ind in range(len(validation_generator_svc)):\n",
    "    images, labels = next(validation_generator_svc)\n",
    "    print(ind, images.shape)\n",
    "    batch = images[:, :, :, 0]\n",
    "    image_array = np.reshape(batch, (images.shape[0], images.shape[1] * images.shape[2]))\n",
    "    %time predictions = model1.predict(image_array)\n",
    "    y_pred1.extend(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred1 = [0 if item == 1.0 else 1 for item in y_pred1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred1[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_generator.labels[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9555459646094088"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_pred1 == validation_generator.labels) / len(y_pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     1.0000    0.9117    0.9538      1166\n",
      "          1     0.9179    1.0000    0.9572      1151\n",
      "\n",
      "avg / total     0.9592    0.9555    0.9555      2317\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(validation_generator.labels, y_pred1, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1063  103]\n",
      " [   0 1151]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(validation_generator.labels, y_pred1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carregar melhor modelo do notebook 01b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "SIZE = (224, 224)\n",
    "\n",
    "model2 = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Conv2D(16, (3, 3),\n",
    "                         padding='same',\n",
    "                         activation='relu',\n",
    "                         input_shape=(*SIZE, 3)),\n",
    "  MaxPooling2D(pool_size=(2, 2)),\n",
    "  Conv2D(32, (3, 3), padding='same', activation='relu'),\n",
    "  MaxPooling2D(pool_size=(2, 2)),\n",
    "  Dropout(0.2),\n",
    "  Conv2D(64, (3, 3), padding='same', activation='relu'),\n",
    "  MaxPooling2D(pool_size=(2, 2)),\n",
    "  Dropout(0.25),\n",
    "  Conv2D(128, (3, 3), padding='same', activation='relu'),\n",
    "  MaxPooling2D(pool_size=(2, 2)),\n",
    "  Dropout(0.25),\n",
    "  Conv2D(128, (3, 3), activation='relu'),\n",
    "  MaxPooling2D(pool_size=(2, 2)),\n",
    "  Dropout(0.25),\n",
    "  Conv2D(256, (3, 3), activation='relu'),\n",
    "  Flatten(),\n",
    "  Dense(256, activation='relu'),\n",
    "  Dropout(0.4),\n",
    "  Dense(1, activation='sigmoid')\n",
    " \n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.load_weights(os.path.join('..', 'models', 'B3modelweights.17-0.08.hdf5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 2s, sys: 8.05 s, total: 1min 10s\n",
      "Wall time: 22.2 s\n"
     ]
    }
   ],
   "source": [
    "%time y_pred2 = model2.predict(validation_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9620198532585239"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred2_labels = y_pred2.reshape(-1) > 0.5\n",
    "\n",
    "sum(y_pred2_labels == validation_generator.labels) / len(y_pred2_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     1.0000    0.9245    0.9608      1166\n",
      "          1     0.9290    1.0000    0.9632      1151\n",
      "\n",
      "avg / total     0.9647    0.9620    0.9620      2317\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(validation_generator.labels, y_pred2_labels, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1078   88]\n",
      " [   0 1151]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(validation_generator.labels, y_pred2_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carregar melhor modelo do notebook 02c3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.densenet import DenseNet121\n",
    "\n",
    "SIZE = (224, 224)\n",
    "base_model = DenseNet121(weights='imagenet',\n",
    "                         input_shape=(*SIZE, 3), \n",
    "                         include_top=False,\n",
    "                         pooling='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2317 128 18\n",
      "last batch setted elements 2304:2317\n",
      "CPU times: user 14min 10s, sys: 56.1 s, total: 15min 6s\n",
      "Wall time: 2min 35s\n"
     ]
    }
   ],
   "source": [
    "def extract_features(generator, model):\n",
    "    generator.reset()\n",
    "    n_images = len(generator.filenames)\n",
    "    batch_size = generator.batch_size\n",
    "    m = batch_size *  (n_images // batch_size)  # Arredondar para não ficar espaço vazio\n",
    "    m = n_images\n",
    "    n = model.output.shape[1]\n",
    "    features = np.zeros((m, n), np.float32)\n",
    "    y_ = np.zeros((m, 1), np.float32)\n",
    "    print(m, batch_size, m // batch_size)\n",
    "    for ind in range(0, m + 1, batch_size):\n",
    "        batch, y_batch = next(generator)\n",
    "        features_batch = base_model.predict(batch)\n",
    "        posi = ind * batch_size\n",
    "        features[ind: ind + len(y_batch), :] = features_batch\n",
    "        y_[ind: ind + len(y_batch), 0] = y_batch\n",
    "    print('last batch setted elements %s:%s' % (ind, ind + len(y_batch)))\n",
    "    return features, y_\n",
    "\n",
    "%time features_test, y_test = extract_features(validation_generator, base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(units=128,\n",
    "                       activation='relu',\n",
    "                       input_shape=(1024,)))\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "model.add(tf.keras.layers.Dense(units=128,\n",
    "                       activation='relu',\n",
    "                       input_shape=(1024,)))\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = '../models_featureextraction'\n",
    "\n",
    "epoch = 40\n",
    "val_loss = 0.08\n",
    "model.load_weights(\n",
    "    os.path.join(MODEL_DIR,\n",
    "                 'Transfermodelweights02c3.{:02d}-{:.2f}.hdf5'.format(epoch, val_loss)\n",
    "                ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 163 ms, sys: 6.28 ms, total: 169 ms\n",
      "Wall time: 126 ms\n"
     ]
    }
   ],
   "source": [
    "%time y_pred3a = model.predict(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9598618903754855"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred3a_labels = y_pred3a.reshape(-1) > 0.5\n",
    "\n",
    "sum(y_pred3a_labels == validation_generator.labels) / len(y_pred3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.9900    0.9297    0.9589      1166\n",
      "          1     0.9329    0.9904    0.9608      1151\n",
      "\n",
      "avg / total     0.9616    0.9599    0.9598      2317\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(validation_generator.labels, y_pred3a_labels, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1084   82]\n",
      " [  11 1140]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(validation_generator.labels, y_pred3a_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = '../models_featureextraction'\n",
    "\n",
    "epoch = 13\n",
    "val_loss = 0.10\n",
    "model.load_weights(\n",
    "    os.path.join(MODEL_DIR,\n",
    "                 'Transfermodelweights02c3-classweights.{:02d}-{:.2f}.hdf5'.format(epoch, val_loss)\n",
    "                ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 63.9 ms, sys: 5.31 ms, total: 69.2 ms\n",
      "Wall time: 43 ms\n"
     ]
    }
   ],
   "source": [
    "%time y_pred3 = model.predict(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2317, 1)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9529564091497627"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred3_labels = y_pred3.reshape(-1) > 0.5\n",
    "\n",
    "sum(y_pred3_labels == validation_generator.labels) / len(y_pred3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.9656    0.9400    0.9526      1166\n",
      "          1     0.9408    0.9661    0.9533      1151\n",
      "\n",
      "avg / total     0.9533    0.9530    0.9530      2317\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(validation_generator.labels[:len(y_pred3)], y_pred3_labels, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1096   70]\n",
      " [  39 1112]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(validation_generator.labels[:len(y_pred3)], y_pred3_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9401197604790419"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1099 / (1099 + 70)  # Não Vazios classificados \"erradamente\" como vazios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9661164205039097"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1112 / (1112 + 39)  # Vazios classificados \"erradamente\" como não vazios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "acertos1 = y_pred1 == validation_generator.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "acertos2 = y_pred2_labels == validation_generator.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "acertos3 = y_pred3_labels == validation_generator.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "acertos3a = y_pred3a_labels == validation_generator.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9935261113508848"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(acertos1 == acertos2) / len(acertos1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9753992231333621"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(acertos2 == acertos3) / len(acertos2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9689253344842469"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(acertos1 == acertos3) / len(acertos1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9827362969356928"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(acertos3 == acertos3a) / len(acertos3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9909365558912386"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(acertos2 == acertos3a) / len(acertos2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carregar melhor modelo do notebook 04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Activation, Conv2D, \\\n",
    "    BatchNormalization, concatenate, Dense, Dropout, Flatten, Input, MaxPooling2D\n",
    "from PIL import Image\n",
    "\n",
    "SIZE = (224, 224)\n",
    "\n",
    "nuclear_model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Conv2D(16, (3, 3),\n",
    "                         padding='same',\n",
    "                         activation='relu',\n",
    "                         input_shape=(*SIZE, 3)),\n",
    "  MaxPooling2D(pool_size=(2, 2)),\n",
    "  Conv2D(32, (3, 3), padding='same', activation='relu'),\n",
    "  MaxPooling2D(pool_size=(2, 2)),\n",
    "  Dropout(0.2),\n",
    "  Conv2D(64, (3, 3), padding='same', activation='relu'),\n",
    "  MaxPooling2D(pool_size=(2, 2)),\n",
    "  Dropout(0.25),\n",
    "  Conv2D(128, (3, 3), padding='same', activation='relu'),\n",
    "  MaxPooling2D(pool_size=(2, 2)),\n",
    "  Dropout(0.25),\n",
    "  Conv2D(128, (3, 3), activation='relu'),\n",
    "  MaxPooling2D(pool_size=(2, 2)),\n",
    "  Dropout(0.25),\n",
    "  Conv2D(256, (3, 3), activation='relu'),\n",
    "  Flatten(),\n",
    "  Dense(128, activation='relu'),\n",
    "])\n",
    "\n",
    "a_in = Input(shape=(*SIZE, 3))\n",
    "b_in = Input(shape=(*SIZE, 3))\n",
    "a_feat = nuclear_model(a_in)\n",
    "b_feat = nuclear_model(b_in)\n",
    "combined_features = concatenate([a_feat, b_feat], name = 'merge_features')\n",
    "combined_features = Dense(16, activation = 'linear')(combined_features)\n",
    "combined_features = BatchNormalization()(combined_features)\n",
    "combined_features = Activation('relu')(combined_features)\n",
    "combined_features = Dense(4, activation = 'linear')(combined_features)\n",
    "combined_features = BatchNormalization()(combined_features)\n",
    "combined_features = Activation('relu')(combined_features)\n",
    "combined_features = Dense(1, activation = 'sigmoid')(combined_features)\n",
    "similarity_model = tf.keras.Model(inputs = [a_in, b_in], outputs = [combined_features], name = 'Similarity_Model')\n",
    "\n",
    "MODEL_DIR = '../models/siamese'\n",
    "epoch = 6\n",
    "val_loss = 0.24\n",
    "\n",
    "similarity_model.load_weights(os.path.join(\n",
    "    MODEL_DIR, \n",
    "    '04modelweights.{:02d}-{:.2f}.hdf5'.format(epoch, val_loss)\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vazio1 = Image.open(os.path.join(caminho_traint, 'vazio/5aa1a06e2a87954aafdda3bc.jpg'))\n",
    "vazio2 = Image.open(os.path.join(caminho_traint, 'vazio/5aa1cc192a87957e60f49482.jpg'))\n",
    "nvazio1 = Image.open(os.path.join(caminho_traint, 'nvazio/5aa1a5222a87954aaedda574.jpg'))\n",
    "nvazio2 = Image.open(os.path.join(caminho_traint, 'nvazio/5aa1cc732a87957e63f4ba9f.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monta_x1_batch(pil_image, batch_size):\n",
    "    x1 = np.zeros((batch_size, *SIZE, 3), np.float32)\n",
    "    img_array = np.array(pil_image.resize(SIZE, Image.ANTIALIAS)) / 255\n",
    "    for ind in range(batch_size):\n",
    "        x1[ind, :, :, :] = img_array\n",
    "    return x1\n",
    "\n",
    "def get_preds(generator, X_batch):\n",
    "    generator.reset()\n",
    "    y = []\n",
    "    y_pred = []\n",
    "    for r in range(len(validation_generator)):\n",
    "        x2_batch, y_batch = next(validation_generator)\n",
    "        pred_batch = similarity_model.predict([X_batch[:len(x2_batch)], x2_batch])\n",
    "        y.extend(y_batch)\n",
    "        y_pred.extend(list(pred_batch.reshape(-1)))\n",
    "    return y, y_pred\n",
    "\n",
    "def evaluate(generator, pil_image, inverso=False):\n",
    "    X_batch = monta_x1_batch(pil_image, len(generator.filenames))\n",
    "    y_test, y_pred = get_preds(generator, X_batch)\n",
    "    if inverso:\n",
    "        y_labels = np.array(y_pred) > 0.5\n",
    "    else:\n",
    "        y_labels = np.array(y_pred) <= 0.5\n",
    "    print(sum(y_labels == y_test) / len(y_labels))\n",
    "    return y_test, y_pred, y_labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9602934829520933\n",
      "CPU times: user 10.5 s, sys: 2.6 s, total: 13.1 s\n",
      "Wall time: 15.4 s\n"
     ]
    }
   ],
   "source": [
    "%time y_test, y_pred, y_labels = evaluate(validation_generator, vazio1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9602934829520933\n",
      "CPU times: user 9.61 s, sys: 2.07 s, total: 11.7 s\n",
      "Wall time: 14 s\n"
     ]
    }
   ],
   "source": [
    "%time y_test, y_predn, y_labelsn = evaluate(validation_generator, nvazio1, inverso=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0     1.0000    0.9211    0.9589      1166\n",
      "        1.0     0.9260    1.0000    0.9616      1151\n",
      "\n",
      "avg / total     0.9632    0.9603    0.9602      2317\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_labels, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1074   92]\n",
      " [   0 1151]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(validation_generator.labels, y_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0     0.9963    0.9245    0.9591      1166\n",
      "        1.0     0.9287    0.9965    0.9614      1151\n",
      "\n",
      "avg / total     0.9627    0.9603    0.9603      2317\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_labelsn, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1078   88]\n",
      " [   4 1147]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(validation_generator.labels, y_labelsn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_labels_media = np.array(y_pred) < np.array(y_predn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0     0.9972    0.9228    0.9586      1166\n",
      "        1.0     0.9273    0.9974    0.9611      1151\n",
      "\n",
      "avg / total     0.9625    0.9599    0.9598      2317\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_labels_media, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1076   90]\n",
      " [   3 1148]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(validation_generator.labels, y_labels_media))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_gym2",
   "language": "python",
   "name": "venv_gym2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

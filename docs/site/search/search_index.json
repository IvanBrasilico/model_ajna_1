{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Treinamento de modelos de vis\u00e3o computacional Abstract Nestes documentos centralizadas as anota\u00e7\u00f5es e hist\u00f3rico detalhado do treinamento de alguns modelos de vis\u00e3o computacional. Planejamento e metas do projeto em CapstoneProject Detalhes dos modelos desenvolvidos e explora\u00e7\u00e3o de dados e modelos em relat\u00f3rio Organiza\u00e7\u00e3o O trabalho foi dividido em v\u00e1rios notebooks para melhor organiza\u00e7\u00e3o. Estes notebooks est\u00e3o com a seguinte nomenclatura <n\u00famero sequencial t\u00e9cnica/modelo><refinamento>-<descricao>-<base> Ex: 01 - n\u00famero sequencial b - refinamento Transfer Learning - t\u00e9cnica vazios - base de dados Assim: 01-RedeSimples-chestXRay \u00e9 uma rede neural simples para classificar a base chestXRay 01-RedeSimples-vazios \u00e9 uma rede neural simples para classificar a base vazios 01b-RedeSimples-vazios \u00e9 a mesma rede/t\u00e9cnica do 01 mas com algumas modifica\u00e7\u00f5es Modelos/bases Conforme detalhado em CapstoneProject, ser\u00e3o treinadas redes convolucionais simples do zero, modelos sofisticados com transfer learning, e redes siameas. As bases utilizadas ser\u00e3o chestXRay, vazios e ncmsunicos.","title":"Home"},{"location":"#treinamento-de-modelos-de-visao-computacional","text":"","title":"Treinamento de modelos de vis\u00e3o computacional"},{"location":"#abstract","text":"Nestes documentos centralizadas as anota\u00e7\u00f5es e hist\u00f3rico detalhado do treinamento de alguns modelos de vis\u00e3o computacional. Planejamento e metas do projeto em CapstoneProject Detalhes dos modelos desenvolvidos e explora\u00e7\u00e3o de dados e modelos em relat\u00f3rio","title":"Abstract"},{"location":"#organizacao","text":"O trabalho foi dividido em v\u00e1rios notebooks para melhor organiza\u00e7\u00e3o. Estes notebooks est\u00e3o com a seguinte nomenclatura <n\u00famero sequencial t\u00e9cnica/modelo><refinamento>-<descricao>-<base> Ex: 01 - n\u00famero sequencial b - refinamento Transfer Learning - t\u00e9cnica vazios - base de dados Assim: 01-RedeSimples-chestXRay \u00e9 uma rede neural simples para classificar a base chestXRay 01-RedeSimples-vazios \u00e9 uma rede neural simples para classificar a base vazios 01b-RedeSimples-vazios \u00e9 a mesma rede/t\u00e9cnica do 01 mas com algumas modifica\u00e7\u00f5es","title":"Organiza\u00e7\u00e3o"},{"location":"#modelosbases","text":"Conforme detalhado em CapstoneProject, ser\u00e3o treinadas redes convolucionais simples do zero, modelos sofisticados com transfer learning, e redes siameas. As bases utilizadas ser\u00e3o chestXRay, vazios e ncmsunicos.","title":"Modelos/bases"},{"location":"notebooks/","text":"Notebooks 01-Baseline-redesimples-vazio 01-Baseline-redesimples-vazio Rede convolucional bem simples treinada do zero. acc: 0.9551 - val_acc: 0.9564 Este notebook tamb\u00e9m cont\u00e9m visualiza\u00e7\u00f5es para tentar entender melhor o que foi aprendido pela rede. 01b-Baseline-redesimples-vazio-tamanhomaior 01b-Baseline-redesimples-vazio-tamanhomaior Mesma rede convolucional, mas treinada com entrada maior (224x224). O tamanho de entrada \u00e9 o mesmo da maioria dos modelos treinados na imagenet. acc: 0.9589 - val_acc: 0.9616 Em 26/06/2019: Rodada tr\u00eas vezes a sequ\u00eancia acima, 99, 101 e 103 erros de classifica\u00e7\u00e3o (a mudan\u00e7a \u00e9 devido a t\u00e9cnicas de image augmentation). Precis\u00e3o de 100% na classe 0 e recall 91% ou seja 9% de erros tipo II falso negativo (predi\u00e7\u00e3o 1 r\u00f3tulo 0). Analisando visualmente o diret\u00f3rio, pelo menos 25% dos erros s\u00e3o de rotulagem (os cont\u00eaineres realmente n\u00e3o cont\u00e9m carga. Dos 70-75 erros restantes, em 20% do total o cont\u00eainer est\u00e1 escuro, parecendo ter carga de espuma. Em torno de 30% do total tamb\u00e9m h\u00e1 diversos tipos de ru\u00eddos na imagem, desde carretas que invadem a \u00e1rea do cont\u00eainer at\u00e9 borr\u00f5es laterais na imagem, mas n\u00e3o carga. Ent\u00e3o tamb\u00e9m \u00e9 cont\u00eainer efetivamente vazio. Nos erros restantes (apenas 20% de 9%) parece haver erro de classifica\u00e7\u00e3o, mas o cont\u00eainer cont\u00e9m pouca carga. Conclus\u00f5es: * O erro real do algoritmo pode ser de apenas 2-4% e apenas na classe N\u00e3o Vazio. Este erro poderia ser melhorado com melhora no recorte do cont\u00eainer e na limpeza da imagem original. * Dos 9% de erros, 2% s\u00e3o aparentemente \"fraudes\": cont\u00eaineres n\u00e3o continham carga * Dos 9% de erros, 2% podem ser \"fraude\" ou falha no esc\u00e2ner * Necess\u00e1rio proibir carretas que obstruam o cont\u00eainer O algoritmo est\u00e1 tentendo a ignorar cargas de cont\u00eaineres declarados como vazios mas borrados/sujos ou com muito pouca carga ou com carga uniforme de espumas/materias pouco densos. Talvez fosse interessante for\u00e7ar o algoritmo a ser mais tendente a diminuir este erro, mesmo que isto custasse aumento de falso positivo na classe vazio. 02-TransferLearningSimples-vazio 02-TransferLearningSimples-vazio Rede Densenet121, pr\u00e9 treinada na imagenet. acc: 0.9545 - val_acc: 0.7126 Claramente, houve um sobreajuste muito grande. Os erros de classifica\u00e7\u00e3o cometidos s\u00e3o gritantes. Foi realizado fine tunning do \u00faltimo bloco convolucional (conv5): acc: 0.9523 - val_acc: 0.8045 Apesar dos resultados ruins na generaliza\u00e7\u00e3o, necess\u00e1rio explorar mais esta possibilidade. A dificuldade pode ser devido ao bias em textura da imagenet. Note-se que esta base \u00e9 em tons de cinza, e o mais importante \u00e9 a geometria. Imagenet \u00e9 colorida e textura \u00e9 importante. ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness https://arxiv.org/abs/1811.12231 02b-TransferLearningSimplesRegularizer-vazio 02b-TransferLearningSimplesRegularizer-vazio Rede Densenet121, pr\u00e9 treinada na imagenet, com regulariza\u00e7\u00e3o. N\u00e3o houve sucesso neste treinamento, necess\u00e1rio debugar posteriormente 02c-TransferLearningSimplesFeatureExtractionRegularizer-vazio 02c-TransferLearningSimplesFeatureExtractionRegularizer-vazio Rede Densenet121, pr\u00e9 treinada na imagenet, com regulariza\u00e7\u00e3o. acc: 0.9408 - val_acc: 0.9514 Neste caso, se optou por utilizar as camadas pr\u00e9 treinadas para feature extraction, e, foi utilizada Max Pooling na \u00faltima camada em vez de Avg Pooling. Observa\u00e7\u00f5es: Ap\u00f3s a extra\u00e7\u00e3o das features das imagens, o treinamento do classificador \u00e9 centenas de vezes mais r\u00e1pido. Assim, a extra\u00e7\u00e3o separada dos features permitir\u00e1 treinar v\u00e1rios classificadores, fazer grid search e cross validation, entre outros. Conforme demonstrado acima, h\u00e1 entre as imagens da classe nvazio diversos exemplos que parecem da classe vazio. Ou s\u00e3o erros de base ou s\u00e3o exemplos extremamente similares aos vazios. O aprendizado deve melhorar eliminando estes da base. Ser\u00e1 criada uma c\u00f3pia da base sem esses exemplos, para testar os mesmos algoritmos e comparar. 01b-Baseline-redesimples-chestXRay-tamanhomaior H01b-Baseline-redesimples-chestXRay-tamanhomaior Rede convolucional bem simples treinada do zero. Treinamento em 04/09/2019: Foram realizadas v\u00e1rias rodadas(sempre continuando pesos do menor val_loss anterior): A primeira com ImageAugmentation e lr=0.001, melhor acc=0.94 e melhor val_acc=0.82 Mesmo a rede sendo simples, aparenta ligeiro overfitting A segunda com lr=0.0001 e mais \u00e9pocas para os callbacks, melhor acc=0.94 e melhor val_acc=0.83 A terceira sem ImageAugmentation, com lr muito pequena. Embora ImageAugmentation seja uma t\u00e9cnica para reduzir overfitting, e a priori tirar possa parecer contrasenso, apenas para testar se deixar a base de treinamento mais parecida com a de testes reduz erro de generaliza\u00e7\u00e3o, ao menos nesses exemplos e no \"fine tunning\" Conforme a teoria previa, o sobreajuste aumentou. acc foi para 0.96 e val_acc caiu para menos de 0.80 Quarta tentativa, com regulariza\u00e7\u00e3o L1 e L2 na \u00faltima camada e otimizador Adam, pareceu que ia conseguir melhoria, foi expandido o treinamento para 50 \u00e9pocas iniciando com uma lr maior, mas a melhoria foi apenas marginal, com val_acc ensaiando ultrapassar 0.87 mas oscilando bastante Conclus\u00f5es/pr\u00f3ximos passos Tentar aumentar regulariza\u00e7\u00e3o, utilizar keras-tuner Testar modelo pr\u00e9-treinado mais poderoso (TransferLearning) Olhar exemplos de kernel no kaggle com melhor desempenho em busca de id\u00e9ias","title":"Relat\u00f3rio"},{"location":"notebooks/#notebooks","text":"","title":"Notebooks"},{"location":"notebooks/#01-baseline-redesimples-vazio","text":"01-Baseline-redesimples-vazio Rede convolucional bem simples treinada do zero. acc: 0.9551 - val_acc: 0.9564 Este notebook tamb\u00e9m cont\u00e9m visualiza\u00e7\u00f5es para tentar entender melhor o que foi aprendido pela rede.","title":"01-Baseline-redesimples-vazio"},{"location":"notebooks/#01b-baseline-redesimples-vazio-tamanhomaior","text":"01b-Baseline-redesimples-vazio-tamanhomaior Mesma rede convolucional, mas treinada com entrada maior (224x224). O tamanho de entrada \u00e9 o mesmo da maioria dos modelos treinados na imagenet. acc: 0.9589 - val_acc: 0.9616 Em 26/06/2019: Rodada tr\u00eas vezes a sequ\u00eancia acima, 99, 101 e 103 erros de classifica\u00e7\u00e3o (a mudan\u00e7a \u00e9 devido a t\u00e9cnicas de image augmentation). Precis\u00e3o de 100% na classe 0 e recall 91% ou seja 9% de erros tipo II falso negativo (predi\u00e7\u00e3o 1 r\u00f3tulo 0). Analisando visualmente o diret\u00f3rio, pelo menos 25% dos erros s\u00e3o de rotulagem (os cont\u00eaineres realmente n\u00e3o cont\u00e9m carga. Dos 70-75 erros restantes, em 20% do total o cont\u00eainer est\u00e1 escuro, parecendo ter carga de espuma. Em torno de 30% do total tamb\u00e9m h\u00e1 diversos tipos de ru\u00eddos na imagem, desde carretas que invadem a \u00e1rea do cont\u00eainer at\u00e9 borr\u00f5es laterais na imagem, mas n\u00e3o carga. Ent\u00e3o tamb\u00e9m \u00e9 cont\u00eainer efetivamente vazio. Nos erros restantes (apenas 20% de 9%) parece haver erro de classifica\u00e7\u00e3o, mas o cont\u00eainer cont\u00e9m pouca carga. Conclus\u00f5es: * O erro real do algoritmo pode ser de apenas 2-4% e apenas na classe N\u00e3o Vazio. Este erro poderia ser melhorado com melhora no recorte do cont\u00eainer e na limpeza da imagem original. * Dos 9% de erros, 2% s\u00e3o aparentemente \"fraudes\": cont\u00eaineres n\u00e3o continham carga * Dos 9% de erros, 2% podem ser \"fraude\" ou falha no esc\u00e2ner * Necess\u00e1rio proibir carretas que obstruam o cont\u00eainer O algoritmo est\u00e1 tentendo a ignorar cargas de cont\u00eaineres declarados como vazios mas borrados/sujos ou com muito pouca carga ou com carga uniforme de espumas/materias pouco densos. Talvez fosse interessante for\u00e7ar o algoritmo a ser mais tendente a diminuir este erro, mesmo que isto custasse aumento de falso positivo na classe vazio.","title":"01b-Baseline-redesimples-vazio-tamanhomaior"},{"location":"notebooks/#02-transferlearningsimples-vazio","text":"02-TransferLearningSimples-vazio Rede Densenet121, pr\u00e9 treinada na imagenet. acc: 0.9545 - val_acc: 0.7126 Claramente, houve um sobreajuste muito grande. Os erros de classifica\u00e7\u00e3o cometidos s\u00e3o gritantes. Foi realizado fine tunning do \u00faltimo bloco convolucional (conv5): acc: 0.9523 - val_acc: 0.8045 Apesar dos resultados ruins na generaliza\u00e7\u00e3o, necess\u00e1rio explorar mais esta possibilidade. A dificuldade pode ser devido ao bias em textura da imagenet. Note-se que esta base \u00e9 em tons de cinza, e o mais importante \u00e9 a geometria. Imagenet \u00e9 colorida e textura \u00e9 importante. ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness https://arxiv.org/abs/1811.12231","title":"02-TransferLearningSimples-vazio"},{"location":"notebooks/#02b-transferlearningsimplesregularizer-vazio","text":"02b-TransferLearningSimplesRegularizer-vazio Rede Densenet121, pr\u00e9 treinada na imagenet, com regulariza\u00e7\u00e3o. N\u00e3o houve sucesso neste treinamento, necess\u00e1rio debugar posteriormente","title":"02b-TransferLearningSimplesRegularizer-vazio"},{"location":"notebooks/#02c-transferlearningsimplesfeatureextractionregularizer-vazio","text":"02c-TransferLearningSimplesFeatureExtractionRegularizer-vazio Rede Densenet121, pr\u00e9 treinada na imagenet, com regulariza\u00e7\u00e3o. acc: 0.9408 - val_acc: 0.9514 Neste caso, se optou por utilizar as camadas pr\u00e9 treinadas para feature extraction, e, foi utilizada Max Pooling na \u00faltima camada em vez de Avg Pooling. Observa\u00e7\u00f5es: Ap\u00f3s a extra\u00e7\u00e3o das features das imagens, o treinamento do classificador \u00e9 centenas de vezes mais r\u00e1pido. Assim, a extra\u00e7\u00e3o separada dos features permitir\u00e1 treinar v\u00e1rios classificadores, fazer grid search e cross validation, entre outros. Conforme demonstrado acima, h\u00e1 entre as imagens da classe nvazio diversos exemplos que parecem da classe vazio. Ou s\u00e3o erros de base ou s\u00e3o exemplos extremamente similares aos vazios. O aprendizado deve melhorar eliminando estes da base. Ser\u00e1 criada uma c\u00f3pia da base sem esses exemplos, para testar os mesmos algoritmos e comparar.","title":"02c-TransferLearningSimplesFeatureExtractionRegularizer-vazio"},{"location":"notebooks/#01b-baseline-redesimples-chestxray-tamanhomaior","text":"H01b-Baseline-redesimples-chestXRay-tamanhomaior Rede convolucional bem simples treinada do zero. Treinamento em 04/09/2019: Foram realizadas v\u00e1rias rodadas(sempre continuando pesos do menor val_loss anterior): A primeira com ImageAugmentation e lr=0.001, melhor acc=0.94 e melhor val_acc=0.82 Mesmo a rede sendo simples, aparenta ligeiro overfitting A segunda com lr=0.0001 e mais \u00e9pocas para os callbacks, melhor acc=0.94 e melhor val_acc=0.83 A terceira sem ImageAugmentation, com lr muito pequena. Embora ImageAugmentation seja uma t\u00e9cnica para reduzir overfitting, e a priori tirar possa parecer contrasenso, apenas para testar se deixar a base de treinamento mais parecida com a de testes reduz erro de generaliza\u00e7\u00e3o, ao menos nesses exemplos e no \"fine tunning\" Conforme a teoria previa, o sobreajuste aumentou. acc foi para 0.96 e val_acc caiu para menos de 0.80 Quarta tentativa, com regulariza\u00e7\u00e3o L1 e L2 na \u00faltima camada e otimizador Adam, pareceu que ia conseguir melhoria, foi expandido o treinamento para 50 \u00e9pocas iniciando com uma lr maior, mas a melhoria foi apenas marginal, com val_acc ensaiando ultrapassar 0.87 mas oscilando bastante Conclus\u00f5es/pr\u00f3ximos passos Tentar aumentar regulariza\u00e7\u00e3o, utilizar keras-tuner Testar modelo pr\u00e9-treinado mais poderoso (TransferLearning) Olhar exemplos de kernel no kaggle com melhor desempenho em busca de id\u00e9ias","title":"01b-Baseline-redesimples-chestXRay-tamanhomaior"},{"location":"sobre/","text":"Desenvolvido na RFB dentro do escopo do Sistema AJNA Ivan da Silva Bras\u00edlico https://github.com/IvanBrasilico/ Apresentado como Capstone Project no curso de Engenheiro de Machine Learning, Udacity.","title":"Sobre"},{"location":"sobre/#desenvolvido-na-rfb-dentro-do-escopo-do-sistema-ajna","text":"Ivan da Silva Bras\u00edlico https://github.com/IvanBrasilico/ Apresentado como Capstone Project no curso de Engenheiro de Machine Learning, Udacity.","title":"Desenvolvido na RFB dentro do escopo do Sistema AJNA"}]}